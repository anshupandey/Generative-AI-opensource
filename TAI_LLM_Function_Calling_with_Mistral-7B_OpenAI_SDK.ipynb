{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-opensource/blob/main/TAI_LLM_Function_Calling_with_Mistral-7B_OpenAI_SDK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qm8x2zhhd-8"
      },
      "source": [
        "# Integrating tools/function with LLM using Mistral"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why function calling?\n",
        "\n",
        "When working with generative text models, it can be difficult to coerce generative models to give consistent outputs in a structured format such as JSON. Function Calling in Gemini allows you to overcome this limitation by forcing the model to output structured data in the format and schema that you define.\n",
        "\n",
        "You can think of Function Calling as a way to get structured output from user prompts and function definitions, use that structured output to make an API request to an external system, then return the function response to the generative model so that it can generate a natural language summary. In other words, function calling in Gemini helps you go from unstructured text in prompt, to a structured data object, and back to natural language again."
      ],
      "metadata": {
        "id": "JNQShhNqBQmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benefits of Function Calling\n",
        "\n",
        "- **Native framework**: Function Calling is a native framework in Gemini, so there's no need to enable additional APIs or install extra packages.\n",
        "- **Time savings**: No need to write custom code to call, parse, and synthesize information from multiple APIs.\n",
        "- **Simplified interaction with generative AI models**: Gemini handles the complex task of understanding the user's intent, predicting function calls, extracting relevant function parameters, and generating natural language summaries.\n",
        "- **Versatility**: Easily extend capabilities by adding more function calls for different APIs and tailoring it to your needs.\n"
      ],
      "metadata": {
        "id": "1HONIxlaBkvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu6uxcLphsRz",
        "outputId": "fa4663f5-c146-410d-e99a-c7678623d659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.8/328.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTT0Jc-qhVby"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import openai\n",
        "\n",
        "os.environ['TOGETHER_API_KEY'] = '4faeef5f54789f54e13a6e05a6e413be38ba3b30c2059c6a8c9b955a413ce99b'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(\n",
        "  api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
        "  base_url=\"https://api.together.xyz/v1\",\n",
        ")\n",
        "\n",
        "\n",
        "model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "response = client.chat.completions.create(\n",
        "  model=model_name,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a travel agent. Be descriptive and helpful.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about San Francisco\"},\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "CAkE10c6_srN",
        "outputId": "ecc869d8-836c-48c6-fab6-ab0a2a5dafb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Absolutely, I'd be happy to tell you about San Francisco! San Francisco is a vibrant and culturally rich city located on the west coast of the United States, in the state of California. It's known for its steep hills, cool summer fog, and iconic landmarks such as the Golden Gate Bridge, Alcatraz Island, and the cable cars.\n",
            "\n",
            "San Francisco is a city that's full of character and diversity, with a wide range of neighborhoods each with their own unique charm. Some of the most popular areas include Fisherman's Wharf, North Beach, Chinatown, Haight-Ashbury, and the Mission District.\n",
            "\n",
            "Fisherman's Wharf is a bustling waterfront area that's home to seafood restaurants, shops, and attractions such as the sea lion colony at Pier 39 and the Maritime National Historical Park. North Beach is San Francisco's Little Italy, with plenty of Italian restaurants, cafes, and bakeries. Chinatown is one of the oldest and largest Chinatowns in the country, with colorful streets, markets, and dim sum restaurants.\n",
            "\n",
            "Haight-Ashbury is a historic neighborhood that was at the center of the 1960s counterculture movement, and is still home to plenty of hippie vibes and vintage shops. The Mission District is a vibrant and diverse neighborhood with colorful murals, trendy bars and restaurants, and a lively nightlife scene.\n",
            "\n",
            "San Francisco is also known for its cultural institutions, including the San Francisco Museum of Modern Art, the de Young Museum, and the Asian Art Museum. The city is also home to a thriving music scene, with plenty of venues for live music and festivals throughout the year.\n",
            "\n",
            "When it comes to food, San Francisco is a foodie's paradise, with a diverse range of cuisines to choose from. The city is famous for its sourdough bread, clam chowder, and burritos, as well as its farm-to-table restaurants and fresh seafood.\n",
            "\n",
            "Overall, San Francisco is a city that has something for everyone, with its stunning natural beauty, rich cultural history, and vibrant neighborhoods. Whether you're interested in art, music, food, or outdoor activities, you're sure to find something to love in San Francisco.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMs1SliPer2Q"
      },
      "source": [
        "In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call one or many functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the external function to be integrated"
      ],
      "metadata": {
        "id": "vDh5eBaVAD1c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUAVQTH1FCob"
      },
      "outputs": [],
      "source": [
        "# Define the OpenWeatherMap API key\n",
        "OWM_API_KEY = \"29af1cea50a401d8e624eea4660b3f59\"\n",
        "\n",
        "def get_current_weather(location, unit=\"kelvin\"):\n",
        "    \"\"\"\n",
        "    Fetches the current weather information for a given location.\n",
        "\n",
        "    Parameters:\n",
        "    - location: str, the name of the location (e.g., \"Paris\").\n",
        "    - unit: str, the unit of temperature (default is \"kelvin\").\n",
        "\n",
        "    Returns:\n",
        "    - str: JSON formatted string containing weather information.\n",
        "    \"\"\"\n",
        "    # Construct the API request URL\n",
        "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={OWM_API_KEY}\"\n",
        "\n",
        "    # Send the API request\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse the temperature and weather forecast from the response\n",
        "    temp = response.json()['main']['temp']\n",
        "    forecast = [response.json()['weather'][0]['main'], response.json()['weather'][0]['description']]\n",
        "\n",
        "    # Create a dictionary with the weather information\n",
        "    weather_info = {\n",
        "        \"location\": location,\n",
        "        \"temperature\": temp,\n",
        "        \"unit\": 'Kelvin',\n",
        "        \"forecast\": forecast\n",
        "    }\n",
        "\n",
        "    # Return the weather information as a JSON string\n",
        "    return json.dumps(weather_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zvpewsATGB30",
        "outputId": "b31c5d1c-658e-4f3e-bc50-bb851ae2275a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"location\": \"Paris\", \"temperature\": 296.31, \"unit\": \"Kelvin\", \"forecast\": [\"Clouds\", \"broken clouds\"]}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "get_current_weather(\"Paris\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sTPInk2CiVpV",
        "outputId": "657120d8-088d-44c4-806a-165314d06057"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"location\": \"Manila\", \"temperature\": 300.25, \"unit\": \"Kelvin\", \"forecast\": [\"Rain\", \"heavy intensity rain\"]}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "get_current_weather(\"Manila\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OLj1VFabUCR",
        "outputId": "4f22b11f-88db-4eaf-d4ab-f39cc696c0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"8a6aca814d4f6057-ORD\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"eos\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \" AI, or Artificial Intelligence, refers to the development of computer systems or machines that can perform tasks that would normally require human intelligence to accomplish. These tasks can include things like learning, reasoning, problem-solving, perception, and language understanding.\\n\\nThere are different types of AI, ranging from narrow or weak AI, which is designed to perform a specific task, such as voice recognition or image analysis, to general or strong AI, which has the ability to understand, learn, and apply knowledge across a wide range of tasks at a level equal to or beyond that of a human being.\\n\\nAI has the potential to transform many industries and aspects of our lives, from healthcare and transportation to education and entertainment. However, it also raises important ethical and social questions related to issues such as privacy, bias, job displacement, and accountability. As such, the development and deployment of AI must be done responsibly, with careful consideration given to its potential impacts on society.\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": null\n",
            "      },\n",
            "      \"seed\": 9855762843678163000\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1721560716,\n",
            "  \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"service_tier\": null,\n",
            "  \"system_fingerprint\": null,\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 199,\n",
            "    \"prompt_tokens\": 13,\n",
            "    \"total_tokens\": 212\n",
            "  },\n",
            "  \"prompt\": []\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\":\"user\",'content':\"what is AI?\"}]\n",
        "results = client.chat.completions.create(model = model_name, messages = messages)\n",
        "print(results.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l316C1xAb-Qy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define tools/functions for the LLM to use\n",
        "tools = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"get_current_weather\",\n",
        "                \"description\": \"Get the current weather in a given location\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"location\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                        },\n",
        "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\",\"kelvin\"]},\n",
        "                    },\n",
        "                    \"required\": [\"location\"],\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gdvGUJGbheA"
      },
      "outputs": [],
      "source": [
        "def get_response(messages, tools, model=model_name):\n",
        "    \"\"\"\n",
        "    Handles interaction with the LLM, including making function calls if needed.\n",
        "\n",
        "    Parameters:\n",
        "    - messages: list, the conversation history.\n",
        "    - tools: list, the functions/tools available to the LLM.\n",
        "    - model: str, the model name to use (default is model_name).\n",
        "\n",
        "    Returns:\n",
        "    - str: The response from the LLM or function.\n",
        "    \"\"\"\n",
        "    # Create a chat completion with the given messages and tools\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice='auto',\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    #print(response.model_dump_json(indent=2))\n",
        "    # Get the generated response and tool calls (if any)\n",
        "    response = response.choices[0].message\n",
        "    tool_calls = response.tool_calls\n",
        "\n",
        "    try:\n",
        "        if tool_calls:\n",
        "            print(\"Making a function call\")\n",
        "            # Available functions\n",
        "            available_functions = {\n",
        "                \"get_current_weather\": get_current_weather,\n",
        "            }\n",
        "            print(tool_calls)\n",
        "\n",
        "            # Extend conversation with assistant's reply\n",
        "            messages.append(response)\n",
        "\n",
        "            # Handle each function call\n",
        "            for tool_call in tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_to_call = available_functions[function_name]\n",
        "                function_args = json.loads(tool_call.function.arguments)\n",
        "                function_response = function_to_call(**function_args)\n",
        "                messages.append(\n",
        "                    {\n",
        "                        \"tool_call_id\": tool_call.id,\n",
        "                        \"role\": \"tool\",\n",
        "                        \"name\": function_name,\n",
        "                        \"content\": function_response,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            # Get a new response from the model with the function response\n",
        "            second_response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "            )\n",
        "            return second_response\n",
        "        else:\n",
        "            return response.content\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred\", e)\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWL0wVqFdeBm",
        "outputId": "1bb7af43-8c1e-4756-e792-b88c17a19c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AI, or Artificial Intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence. This includes tasks such as learning, problem-solving, decision-making, and understanding natural language.\n"
          ]
        }
      ],
      "source": [
        "# Example usage of get_response function\n",
        "messages = [{\"role\": \"user\", \"content\": \"Provide a 2 line explanation for AI\"}]\n",
        "response = get_response(messages, tools)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0H4DaivdmPW",
        "outputId": "73c12aa1-cfe1-4afd-aa06-732d2a646ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making a function call\n",
            "[ChatCompletionMessageToolCall(id='call_md194vfg929glspn145htd7c', function=Function(arguments='{\"location\":\"Tokyo\"}', name='get_current_weather'), type='function')]\n",
            " The current temperature in Tokyo is 303.4 Kelvin, which is equivalent to 30.2 degrees Celsius or 86.4 degrees Fahrenheit. The forecast shows broken clouds. Please note that weather conditions can change rapidly, so it's always a good idea to check the most recent weather updates.\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"How is the weather in Tokyo today?\"}]\n",
        "response = get_response(messages, tools)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iaZlvyyQk8SW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}