{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnU7crJ3Eg12998jqngLNv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4acdefd924fa4b138afd91d4698808dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8f2f38996b542548162a4a1f1f04025",
              "IPY_MODEL_a1988a431ba64713ace3b698de2b7f1f",
              "IPY_MODEL_669b9e5c09dc4314a16a13b65c6d338a"
            ],
            "layout": "IPY_MODEL_53129e5380644b6aa0008021729f23a6"
          }
        },
        "c8f2f38996b542548162a4a1f1f04025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61ce5c4568a046ecb1fa6e2887f70c20",
            "placeholder": "​",
            "style": "IPY_MODEL_fcb1d64b7895473aa58bf2290f23103e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a1988a431ba64713ace3b698de2b7f1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71421fb0acc0460b8f63f8e2c00419ef",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ead7a9edcbd4a24ae5db89c20380431",
            "value": 2
          }
        },
        "669b9e5c09dc4314a16a13b65c6d338a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4b8e3ac534f402fb7e1164ab93136d9",
            "placeholder": "​",
            "style": "IPY_MODEL_6007cf94a2924efb86f0024f7a889b63",
            "value": " 2/2 [00:19&lt;00:00,  8.30s/it]"
          }
        },
        "53129e5380644b6aa0008021729f23a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61ce5c4568a046ecb1fa6e2887f70c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcb1d64b7895473aa58bf2290f23103e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71421fb0acc0460b8f63f8e2c00419ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ead7a9edcbd4a24ae5db89c20380431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4b8e3ac534f402fb7e1164ab93136d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6007cf94a2924efb86f0024f7a889b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Generative-AI-opensource/blob/main/LLM_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk rouge-score --quiet"
      ],
      "metadata": {
        "id": "rJ1SBbG5xpjw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics for Language Models\n",
        "\n",
        "## Perplexity\n",
        "\n",
        "**Definition**: Perplexity is a measure of how well a language model predicts a sample. A lower perplexity indicates that the model is better at predicting the sample.\n",
        "\n",
        "**Interpretation**:\n",
        "- **Low Perplexity**: Indicates better performance (e.g., 10 or lower).\n",
        "- **High Perplexity**: Indicates worse performance.\n",
        "\n",
        "**Benchmark**: For modern language models, perplexity values typically range between 10 and 50 for well-formed English text.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uxVPzUNxz9lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "id": "_O2hs1MLzSsd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "predictions = [\"the cat is on the the mat in house in the city\", \"there is a cat on the the mat in the garage\"]\n",
        "references = [[\"the cat is on the mat\"], [\"there is a cat on the mat\"]]"
      ],
      "metadata": {
        "id": "PxgV9Ug2zU9Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the = 1/(2/6) = 1/(1/3) = 1/0.33 = 3 ||||   1/2^3 = 1/8\n",
        "# cat = 1/(1/6) = 6  ||| 1/2^6 = 1/64"
      ],
      "metadata": {
        "id": "vWozhEQE9oO_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "references[0][0].split()"
      ],
      "metadata": {
        "id": "ggpakUPe9Fhx",
        "outputId": "76ea74dd-7a4c-46d0-ece5-e1f83ec33cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'cat', 'is', 'on', 'the', 'mat']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity Calculation\n",
        "def calculate_perplexity(predicted_sentence, reference_sentence):\n",
        "    ref_len = len(reference_sentence.split()) # calculating total num of words in reference\n",
        "    log_prob_sum = 0\n",
        "    for word in reference_sentence.split():\n",
        "        if word in predicted_sentence.split():\n",
        "            log_prob_sum += math.log(1 / (predicted_sentence.split().count(word) / len(predicted_sentence.split())))\n",
        "        else:\n",
        "            log_prob_sum += math.log(1 / len(predicted_sentence.split()))\n",
        "    return math.exp(log_prob_sum / ref_len)\n",
        "\n",
        "perplexities = [calculate_perplexity(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_perplexity = sum(perplexities) / len(perplexities)\n",
        "print(f\"Average Perplexity: {average_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR-f9jpszV8b",
        "outputId": "ff1c4ee8-b60a-40f9-eb72-3560bc2063b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Perplexity: 8.480895849173958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "predictions = [\"the cat is on the mat\", \"there is a cat on the the mat in the garage\"]\n",
        "references = [[\"the cat is on the mat\"], [\"there is a cat on the mat\"]]"
      ],
      "metadata": {
        "id": "3qxMPFKJD-wv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BLEU (Bilingual Evaluation Understudy)\n",
        "\n",
        "**Definition**: BLEU is a metric for evaluating a generated sentence to a reference sentence. It measures the n-gram precision with a penalty for overly short sentences.\n",
        "\n",
        "**Interpretation**:\n",
        "- **High BLEU Score**: Indicates good performance (e.g., scores above 0.5 or 50%).\n",
        "- **Low BLEU Score**: Indicates poor performance.\n",
        "\n",
        "**Benchmark**: For machine translation tasks, a BLEU score above 0.3 (30%) is considered reasonable, while scores above 0.5 (50%) are considered good.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "L8ce5bPbUqgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU Score Calculation\n",
        "def calculate_bleu(predicted_sentence, reference_sentence):\n",
        "    return sentence_bleu([reference_sentence.split()], predicted_sentence.split())\n",
        "\n",
        "bleu_scores = [calculate_bleu(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(f\"Average BLEU Score: {average_bleu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU-Bq3e0zjfL",
        "outputId": "a05fd752-5fa6-43a9-e4fc-05c1416e0355"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 0.751128696864156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
        "\n",
        "**Definition**: ROUGE measures the overlap of n-grams between the generated sentence and the reference sentence, focusing on recall.\n",
        "\n",
        "**Types**:\n",
        "- **ROUGE-1**: Measures the overlap of unigrams.\n",
        "- **ROUGE-2**: Measures the overlap of bigrams.\n",
        "- **ROUGE-L**: Measures the longest common subsequence.\n",
        "\n",
        "**Interpretation**:\n",
        "- **High ROUGE Score**: Indicates good performance.\n",
        "- **Low ROUGE Score**: Indicates poor performance.\n",
        "\n",
        "**Benchmark**: For summarization tasks, ROUGE scores of 0.5 (50%) or higher are considered good.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dWBJ4fIvUuTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE Score Calculation\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def calculate_rouge(predicted_sentence, reference_sentence):\n",
        "    scores = scorer.score(reference_sentence, predicted_sentence)\n",
        "    return scores\n",
        "\n",
        "rouge_scores = [calculate_rouge(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "\n",
        "average_rouge = {\n",
        "    'rouge1': sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rouge2': sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rougeL': sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "}\n",
        "\n",
        "print(f\" Average ROUGE Scores: {average_rouge}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSMlLsJazmDh",
        "outputId": "0e17bd55-0c4b-46cf-a13a-f8adb415ec0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Average ROUGE Scores: {'rouge1': 0.8888888888888888, 'rouge2': 0.875, 'rougeL': 0.8888888888888888}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "inputs = [\n",
        "    \"Translate the following English text to French: 'Hello, how are you?'\",\n",
        "    \"Summarize the following text wihtout loosing context: 'The quick brown fox jumps over the lazy dog.'\"\n",
        "]\n",
        "references = [\n",
        "    [\"Bonjour, comment ça va?\"],\n",
        "    [\"The quick brown fox jumps over the lazy dog.\"]\n",
        "]"
      ],
      "metadata": {
        "id": "R-7XSNqdxoh5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DOXSfnNz2Ke"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Gemma 2 model"
      ],
      "metadata": {
        "id": "rMLk7oCXWzlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: For any error related to memory, restart and run all"
      ],
      "metadata": {
        "id": "srlzqEmGO1lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Write a poem on city Dubai in 1000 words?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"google/gemma-2b-it\",device=0)\n",
        "response = pipe(messages,max_length=2000)\n",
        "response"
      ],
      "metadata": {
        "id": "3I72psdAyQYF",
        "outputId": "f660d5d8-72cf-46fb-ebfe-dfe41ee4f8d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486,
          "referenced_widgets": [
            "4acdefd924fa4b138afd91d4698808dc",
            "c8f2f38996b542548162a4a1f1f04025",
            "a1988a431ba64713ace3b698de2b7f1f",
            "669b9e5c09dc4314a16a13b65c6d338a",
            "53129e5380644b6aa0008021729f23a6",
            "61ce5c4568a046ecb1fa6e2887f70c20",
            "fcb1d64b7895473aa58bf2290f23103e",
            "71421fb0acc0460b8f63f8e2c00419ef",
            "3ead7a9edcbd4a24ae5db89c20380431",
            "b4b8e3ac534f402fb7e1164ab93136d9",
            "6007cf94a2924efb86f0024f7a889b63"
          ]
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4acdefd924fa4b138afd91d4698808dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'user',\n",
              "    'content': 'Write a poem on city Dubai in 1000 words?'},\n",
              "   {'role': 'assistant',\n",
              "    'content': \"A mirage in the desert's embrace,\\nDubai, a city that never sleeps.\\nTowers pierce the sky, a symphony of steel,\\nA testament to ambition, a beacon of wealth.\\n\\nGlittering skyscrapers, reaching for the sky,\\nA canvas of glass, where dreams can fly.\\nFrom Burj Khalifa's spire, a crown upon the land,\\nTo Dubai Fountain's dance, a mesmerizing stand.\\n\\nA city of contrasts, old and new,\\nA blend of tradition and modern lore.\\nPalm Jumeirah's crescent, a marvel to behold,\\nA haven of luxury, a story to be told.\\n\\nThe Dubai souks, a vibrant display,\\nWhere treasures from every land come to play.\\nSpice and spices, textiles so fine,\\nA cultural tapestry, a vibrant line.\\n\\nThe Dubai Mall, a shopper's delight,\\nWhere luxury brands ignite the night.\\nThe Burj Al Arab, a haven of grace,\\nA timeless landmark, a timeless space.\\n\\nThe Dubai Fountain show, a symphony of light,\\nA spectacle that captivates the night.\\nThe Dubai Miracle Garden, a floral delight,\\nWhere nature's wonders illuminate the night.\\n\\nA city of ambition, a place to be seen,\\nA testament to human will, a beacon of keen.\\nFrom the desert dunes to the azure seas,\\nDubai's allure, a treasure that cannot be beat.\\n\\nSo come, explore this city, so grand and tall,\\nA city that's alive, a story that's enthrall.\\nDubai, a marvel, a city of dreams,\\nA testament to human spirit, a world without seams.\"}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0]['generated_text'][1]['content'])"
      ],
      "metadata": {
        "id": "_2IxdMevuaNz",
        "outputId": "6d6030e7-9f02-4e9c-ce57-5a99797db381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A mirage in the desert's embrace,\n",
            "Dubai, a city that never sleeps.\n",
            "Towers pierce the sky, a symphony of steel,\n",
            "A testament to ambition, a beacon of wealth.\n",
            "\n",
            "Glittering skyscrapers, reaching for the sky,\n",
            "A canvas of glass, where dreams can fly.\n",
            "From Burj Khalifa's spire, a crown upon the land,\n",
            "To Dubai Fountain's dance, a mesmerizing stand.\n",
            "\n",
            "A city of contrasts, old and new,\n",
            "A blend of tradition and modern lore.\n",
            "Palm Jumeirah's crescent, a marvel to behold,\n",
            "A haven of luxury, a story to be told.\n",
            "\n",
            "The Dubai souks, a vibrant display,\n",
            "Where treasures from every land come to play.\n",
            "Spice and spices, textiles so fine,\n",
            "A cultural tapestry, a vibrant line.\n",
            "\n",
            "The Dubai Mall, a shopper's delight,\n",
            "Where luxury brands ignite the night.\n",
            "The Burj Al Arab, a haven of grace,\n",
            "A timeless landmark, a timeless space.\n",
            "\n",
            "The Dubai Fountain show, a symphony of light,\n",
            "A spectacle that captivates the night.\n",
            "The Dubai Miracle Garden, a floral delight,\n",
            "Where nature's wonders illuminate the night.\n",
            "\n",
            "A city of ambition, a place to be seen,\n",
            "A testament to human will, a beacon of keen.\n",
            "From the desert dunes to the azure seas,\n",
            "Dubai's allure, a treasure that cannot be beat.\n",
            "\n",
            "So come, explore this city, so grand and tall,\n",
            "A city that's alive, a story that's enthrall.\n",
            "Dubai, a marvel, a city of dreams,\n",
            "A testament to human spirit, a world without seams.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "qPmrRQRkXScO",
        "outputId": "1b41fa98-bfc9-4b57-8f99-e68aa0d7bf4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Translate the following English text to French: 'Hello, how are you?'\", \"Summarize the following text wihtout loosing context: 'The quick brown fox jumps over the lazy dog.'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lslYAvw37JGQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_response(prompt,pipe=pipe):\n",
        "  messages = [{\"role\": \"user\", \"content\": prompt},]\n",
        "  response = pipe(messages,max_length=2000)\n",
        "  return response[0]['generated_text'][1]['content']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from Gemini\n",
        "predictions = [generate_response(input_text) for input_text in inputs]"
      ],
      "metadata": {
        "id": "AAtsTC5pzHji"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "NeTIOfjBXNDk",
        "outputId": "1ef53cb9-971b-4ee0-8724-e80a660f5c80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sure, here is the French translation of the English text \"Hello, how are you?\":\\n\\n**French:** Salut, comment allez-vous?',\n",
              " \"Sure, here's a summary of the text without losing context:\\n\\nThe quick brown fox jumps over the lazy dog.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(references)):\n",
        "  print(f\"Reference {i+1}: {references[i][0]}\")\n",
        "  print(f\"Prediction {i+1}: {predictions[i]}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "Jmpe2kayIbW_",
        "outputId": "d671f325-1a8b-4a42-8ad9-8cb093d87448",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1: Bonjour, comment ça va?\n",
            "Prediction 1: Sure, here is the French translation of the English text \"Hello, how are you?\":\n",
            "\n",
            "**French:** Salut, comment allez-vous?\n",
            "\n",
            "Reference 2: The quick brown fox jumps over the lazy dog.\n",
            "Prediction 2: Sure, here's a summary of the text without losing context:\n",
            "\n",
            "The quick brown fox jumps over the lazy dog.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity\n",
        "perplexities = [calculate_perplexity(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_perplexity = sum(perplexities) / len(perplexities)\n",
        "print(f\"Average Perplexity: {average_perplexity}\")"
      ],
      "metadata": {
        "id": "tzkDA9FTz3TX",
        "outputId": "95e0e5af-2e90-4672-ed0a-dc440ffaaeff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Perplexity: 8.913660896927015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # BLEU Score Calculation\n",
        "bleu_scores = [calculate_bleu(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(f\"Average BLEU Score: {average_bleu}\")"
      ],
      "metadata": {
        "id": "F5_zOrk4IZwt",
        "outputId": "b33e9a55-b8d1-486d-ba27-2cb75f606750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 0.21230816589401721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE Score\n",
        "rouge_scores = [calculate_rouge(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_rouge = {\n",
        "    'rouge1': sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rouge2': sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rougeL': sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "}\n",
        "print(f\"ROUGE 1 Score: {average_rouge['rouge1']}\")\n",
        "print(f\"ROUGE 2 Score: {average_rouge['rouge2']}\")\n",
        "print(f\"ROUGE L Score: {average_rouge['rougeL']}\")"
      ],
      "metadata": {
        "id": "15tm0vg9IutO",
        "outputId": "e0800182-7457-4e5e-b5d5-f3e56c57fd30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE 1 Score: 0.35382308845577215\n",
            "ROUGE 2 Score: 0.2962962962962963\n",
            "ROUGE L Score: 0.35382308845577215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_oRTiqyJNLy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "Here’s a quick summary of the metrics:\n",
        "\n",
        "| Metric      | Definition                                                                                       | Interpretation                                   | Benchmark                                 |\n",
        "|-------------|--------------------------------------------------------------------------------------------------|-------------------------------------------------|-------------------------------------------|\n",
        "| **Perplexity** | Measures how well a model predicts a sample. Lower values are better.                            | Low = Better (e.g., 10 or lower), High = Worse  | 10-50 for well-formed English text         |\n",
        "| **BLEU**      | Evaluates generated sentence against reference. Measures n-gram precision with penalty for short sentences. | High = Better (e.g., > 0.5), Low = Worse         | > 0.3 is reasonable, > 0.5 is good         |\n",
        "| **ROUGE**     | Measures n-gram overlap between generated and reference sentences. Focuses on recall.            | High = Better (e.g., > 0.5), Low = Worse         | > 0.5 for summarization tasks              |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bWeTI6RZUy0m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FC6S0HKGU0rv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}